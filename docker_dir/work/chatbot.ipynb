{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = {\n",
    "    \"where_to_have_lunch\": \" \".join([\n",
    "        \"where is a good place to have lunch?\",\n",
    "        \"where can i eat?\",\n",
    "        \"what's good for lunch?\",\n",
    "        \"I'm hungry\",\n",
    "        \"anything nice for lunch?\",\n",
    "        \"what's a good place for lunch\"\n",
    "    ]),\n",
    "    \"where_to_have_tea\": \" \".join([\n",
    "        \"it's 4pm, time for tea!\",\n",
    "        \"any good coffee places\",\n",
    "        \"where to have tea\",\n",
    "        \"where to have coffee\",\n",
    "        \"what's for tea\",\n",
    "        \"I would like to go for a coffee break\"\n",
    "    ]),\n",
    "    \"time_for_bed\": \" \".join([\n",
    "        \"lights out!\",\n",
    "        \"time to go to sleep\",\n",
    "        \"sleep time\",\n",
    "        \"time for bed\",\n",
    "        \"its late, time to go to sleep\"\n",
    "    ])\n",
    "}\n",
    "\n",
    "output_answers = {\n",
    "    \"where_to_have_lunch\": \"here's a chinese restaurant that you always go to!\",\n",
    "    \"where_to_have_tea\": \"starbucks near roppongi\",\n",
    "    \"time_for_bed\": \"lights out!\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.6/site-packages (3.4.5)\n",
      "Requirement already satisfied: spacy in /opt/conda/lib/python3.6/site-packages (2.1.8)\n",
      "Requirement already satisfied: spacy_hunspell in /opt/conda/lib/python3.6/site-packages (0.1.0)\n",
      "Requirement already satisfied: hunspell in /opt/conda/lib/python3.6/site-packages (0.5.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from nltk) (1.12.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from spacy) (2.0.2)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /opt/conda/lib/python3.6/site-packages (from spacy) (0.9.6)\n",
      "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /opt/conda/lib/python3.6/site-packages (from spacy) (0.2.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.6/site-packages (from spacy) (2.21.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.6/site-packages (from spacy) (1.17.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.6/site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /opt/conda/lib/python3.6/site-packages (from spacy) (0.1.0)\n",
      "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /opt/conda/lib/python3.6/site-packages (from spacy) (2.0.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /opt/conda/lib/python3.6/site-packages (from spacy) (0.2.2)\n",
      "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /opt/conda/lib/python3.6/site-packages (from spacy) (7.0.8)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.1)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2018.11.29)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /opt/conda/lib/python3.6/site-packages (from thinc<7.1.0,>=7.0.8->spacy) (4.36.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk spacy spacy_hunspell hunspell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0\n",
      "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz (11.1MB)\n",
      "\u001b[K    100% |████████████████████████████████| 11.1MB 2.5MB/s ta 0:00:01  5% |█▉                              | 645kB 557kB/s eta 0:00:19    16% |█████▎                          | 1.8MB 2.1MB/s eta 0:00:05    25% |████████                        | 2.8MB 719kB/s eta 0:00:12    51% |████████████████▍               | 5.7MB 1.7MB/s eta 0:00:04    55% |█████████████████▊              | 6.1MB 1.3MB/s eta 0:00:04    58% |██████████████████▋             | 6.4MB 3.5MB/s eta 0:00:02    66% |█████████████████████▍          | 7.4MB 1.5MB/s eta 0:00:03    67% |█████████████████████▋          | 7.5MB 1.1MB/s eta 0:00:04    75% |████████████████████████▏       | 8.4MB 1.8MB/s eta 0:00:02    84% |███████████████████████████     | 9.3MB 1.4MB/s eta 0:00:02\n",
      "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
      "  Running setup.py install for en-core-web-sm ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed en-core-web-sm-2.1.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training\n",
    "\n",
    "1. separate values form labels\n",
    "2. turn values into vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "# exclude stop words\n",
    "from nltk.corpus import stopwords\n",
    "# word similarity\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "# typo correction - https://github.com/tokestermw/spacy_hunspell\n",
    "from spacy_hunspell import spaCyHunSpell\n",
    "\n",
    "nlp = en_core_web_sm.load()\n",
    "hunspell_dicts = (\"/home/jovyan/hunspell_dictionaries/en_US.dic\", \"/home/jovyan/hunspell_dictionaries/en_US.aff\")\n",
    "hunspell = spaCyHunSpell(nlp, hunspell_dicts) # stupid me, this is a linux docker, don'T use mac\n",
    "nlp.add_pipe(hunspell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original:    I can haz cheezeburger.\n",
      "spellcheck:  True True False False True\n",
      "\n",
      "is the spelling 'haz' correct? False\n",
      "suggested spellings:  ha, haze, hazy, has, hat, had, hag, ham, hap, hay, haw, ha z\n",
      "\n",
      "is the spelling 'cheezburger' correct? False\n",
      "suggested spellings:  cheeseburger, vegeburger\n"
     ]
    }
   ],
   "source": [
    "# sanity test for spacy and hunspell\n",
    "doc = nlp('I can haz cheezeburger.')\n",
    "\n",
    "print(\"original:   \", doc)\n",
    "spellcheck = \" \".join([str(word._.hunspell_spell) for word in doc])\n",
    "print(\"spellcheck: \", spellcheck)\n",
    "\n",
    "haz = doc[2]\n",
    "chk = haz._.hunspell_spell  # False\n",
    "print(\"\\nis the spelling 'haz' correct?\", chk)\n",
    "print(\"suggested spellings: \", \", \".join(haz._.hunspell_suggest))\n",
    "\n",
    "chz = doc[3]\n",
    "chk = chz._.hunspell_spell\n",
    "print(\"\\nis the spelling 'cheezburger' correct?\", chk)\n",
    "print(\"suggested spellings: \", \", \".join(chz._.hunspell_suggest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None,\n",
      "        stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',... 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"],\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# preprocess and vectorize word\n",
    "training_documents = list(questions.values())\n",
    "labels = list(questions.keys())\n",
    "\n",
    "# remove stopwords (unimportant words)\n",
    "en_stop_words = stopwords.words(\"english\")\n",
    "\n",
    "# vectorize\n",
    "vectorizer = CountVectorizer(stop_words=en_stop_words) # play with vectorizer params\n",
    "print(vectorizer)\n",
    "X = vectorizer.fit_transform(training_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"where is a good place to have lunch? where can i eat? what's good for lunch? I'm hungry anything nice for lunch? what's a good place for lunch\",\n",
       " \"it's 4pm, time for tea! any good coffee places where to have tea where to have coffee what's for tea I would like to go for a coffee break\",\n",
       " 'lights out! time to go to sleep sleep time time for bed its late, time to go to sleep']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "  (0, 13)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 8)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 12)\t4\n",
      "  (0, 14)\t2\n",
      "  (0, 7)\t3\n",
      "  (1, 3)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 11)\t1\n",
      "  (1, 19)\t1\n",
      "  (1, 15)\t1\n",
      "  (1, 4)\t3\n",
      "  (1, 17)\t3\n",
      "  (1, 18)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 7)\t1\n",
      "  (2, 9)\t1\n",
      "  (2, 2)\t1\n",
      "  (2, 16)\t3\n",
      "  (2, 10)\t1\n",
      "  (2, 6)\t2\n",
      "  (2, 18)\t4\n",
      "['4pm', 'anything', 'bed', 'break', 'coffee', 'eat', 'go', 'good', 'hungry', 'late', 'lights', 'like', 'lunch', 'nice', 'place', 'places', 'sleep', 'tea', 'time', 'would']\n"
     ]
    }
   ],
   "source": [
    "# these are our feature vectors and their labels\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "print(len(feature_names))\n",
    "print(X)\n",
    "print(feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model\n",
    "\n",
    "classifier = MultinomialNB() # play with models\n",
    "classifier.fit(X, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.33333333  0.33333333  0.33333333]\n",
      " [ 0.33333333  0.33333333  0.33333333]\n",
      " [ 0.33333333  0.33333333  0.33333333]\n",
      " [ 0.33333333  0.33333333  0.33333333]]\n",
      "['time_for_bed' 'time_for_bed' 'time_for_bed' 'time_for_bed']\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "\n",
    "def predict(raw_queries):\n",
    "    queries = vectorizer.transform(raw_queries)\n",
    "    prob = classifier.predict_proba(queries)\n",
    "    print(prob)\n",
    "    return classifier.predict(queries)\n",
    "\n",
    "raw_queries = [\" it's quite noon now please\", \"what for eats\", \"hello\", \"night!\"]\n",
    "\n",
    "print(predict(raw_queries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['where_to_have_lunch', 'where_to_have_lunch', 'time_for_bed'],\n",
       "      dtype='<U19')"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.33333333  0.33333333  0.33333333]\n",
      " [ 0.33333333  0.33333333  0.33333333]\n",
      " [ 0.33333333  0.33333333  0.33333333]\n",
      " [ 0.33333333  0.33333333  0.33333333]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# accuracy\n",
    "# compare input with expected classes\n",
    "\n",
    "tests = [\" it's quite noon now please\", \"what for eats\", \"hello\", \"night!\"]\n",
    "expected = [\"where_to_have_lunch\", \"where_to_have_lunch\", \"none\", \"time_for_bed\"]\n",
    "predicted = predict(tests)\n",
    "\n",
    "# has interesting params to play with\n",
    "evaluation = precision_recall_fscore_support(expected, predicted)\n",
    "metrics = {}\n",
    "(metrics[\"p\"], metrics[\"r\"], metrics[\"f1\"], _) = evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'p': array([ 0.  ,  0.25,  0.  ]),\n",
       " 'r': array([ 0.,  1.,  0.]),\n",
       " 'f1': array([ 0. ,  0.4,  0. ])}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# challenges\n",
    "1. return the answer\n",
    "2. exclude stop words (unimportant words)\n",
    "3. handle synonyms (e.g. lobby == front desk)\n",
    "4. handle typos\n",
    "5. return unknown\n",
    "    - if the questions is outside the domain of the chatbot, defer\n",
    "6. handle paramenter (e.g. set my check out time to 3pm)\n",
    "    - extract 3pm as parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.14728275  0.71409819  0.13861906]\n",
      " [ 0.14790403  0.57368837  0.27840759]\n",
      " [ 0.63677639  0.12349603  0.23972758]]\n",
      "\n",
      "predictions\n",
      "where_to_have_lunch -> here's a chinese restaurant that you always go to!\n",
      "where_to_have_lunch -> here's a chinese restaurant that you always go to!\n",
      "time_for_bed -> lights out!\n"
     ]
    }
   ],
   "source": [
    "# return the answer\n",
    "def answer(raw_queries):\n",
    "    predictions = predict(raw_queries)\n",
    "    answers = [output_answers[p] for p in predictions]\n",
    "    \n",
    "    print('\\npredictions')\n",
    "    for i, p in enumerate(predictions):\n",
    "        print(p + \" -> \" + answers[i])\n",
    "\n",
    "    return answers\n",
    "queries = [\"what's for lunch\", \"good day today\", \"evening, time for some snores\"]\n",
    "\n",
    "results = answer(queries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude stop words above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle synonyms and typos using vector models?\n",
    "\n",
    "\n",
    "# Tokenization:\n",
    "# Using the spaCy library, individual words and punctuation marks were identified\n",
    "# Each word was sent to the Hunspell library and if it is misspelled, the tool. provides a list of suggested replacements. If there were no replacements, the query would be forwarded to the FAQ module without any corrections.\n",
    "\n",
    "# Similarity matching:\n",
    "# The suggested replacements were converted to word vectors using spaCy.\n",
    "# These vectors were then compared to the vector representation of the original misspelled word.\n",
    "# The comparison generates an initial similarity score that indicates how similar the suggestion is to the original word.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return unknown\n",
    "\n",
    "#predictor returns top 10 intents\n",
    "#if the score is too close together, the chatbot might not know what the actual intent is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# other challenges\n",
    "\n",
    "1. other languages\n",
    "2. conjugation and punctuation (word stemming)\n",
    "3. domination of frequent words or intents\n",
    "4. conversation state\n",
    "5. conversation design\n",
    "6. interface for data entry (customizing the chatbot)\n",
    "7. dealing with what is happening when it goes wrong\n",
    "8. recognizing different entities in the sentence and deciding whether the noun or article is important for this intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
